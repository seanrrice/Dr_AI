{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a7c372a-2c47-45ef-841d-961ae372c75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cu126\n",
      "12.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68877072-44f0-48c3-a0bf-2deb31756705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete Training Script for Facial Emotion Recognition using ResNet-34\n",
    "5 emotions: angry, disgust, happy, low affect (neutral & sad), arousal (fear & suprise)\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "from torchvision import transforms, datasets\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score  #accuracy score used?\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Paths - MODIFY THESE\n",
    "DATA_DIR = Path(r\"C:\\Users\\Dr_AI.TWR\\fer2013Prototype\\emotion_pipeline\\master_dataset\\dataset\") # Directory with 7 emotion folders\n",
    "\n",
    "#Ensure path exists\n",
    "assert DATA_DIR.exists(), f\"DATA_DIR not found: {DATA_DIR}\"\n",
    "\n",
    "#Merge neutral/sad, fear/surprise\n",
    "NEW_CLASSES = [\"Angry\", \"Disgust\", \"Happy\", \"LowAffect\", \"Arousal\"]\n",
    "\n",
    "MERGE_MAP = {\n",
    "    \"Angry\": \"Angry\",\n",
    "    \"Disgust\": \"Disgust\",\n",
    "    \"Happy\": \"Happy\",\n",
    "    \"Neutral\": \"LowAffect\",\n",
    "    \"Sad\": \"LowAffect\",\n",
    "    \"Fear\": \"Arousal\",\n",
    "    \"Surprise\": \"Arousal\",\n",
    "}\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3       # This name might be misleading bc of variable learning rate (check later)\n",
    "NUM_WORKERS = 0            #set to 0 if you get issues on Windows\n",
    "VAL_SPLIT = 0.15\n",
    "TRAIN_SPLIT = 0.70\n",
    "PATIENCE = 7  # For early stopping\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 42\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "819bd11d-cfc7-4a6a-a9b7-bee19bc03ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergedImageFolder(Dataset):\n",
    "    def __init__(self, root, merge_map, class_names, transform=None):\n",
    "        self.base = datasets.ImageFolder(root=root)  # original folder labels\n",
    "        self.merge_map = merge_map\n",
    "        self.transform = transform\n",
    "\n",
    "        # merged class interface (ImageFolder-like)\n",
    "        self.classes = list(class_names)\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
    "\n",
    "        # original class index -> name\n",
    "        idx_to_class = {v: k for k, v in self.base.class_to_idx.items()}\n",
    "\n",
    "        # remap: old_idx -> new_idx\n",
    "        self.remap = {}\n",
    "        for old_idx, old_name in idx_to_class.items():\n",
    "            if old_name not in self.merge_map:\n",
    "                raise KeyError(f\"Missing '{old_name}' in merge_map keys: {list(self.merge_map.keys())}\")\n",
    "            new_name = self.merge_map[old_name]\n",
    "            if new_name not in self.class_to_idx:\n",
    "                raise KeyError(f\"merge_map maps '{old_name}' -> '{new_name}', but '{new_name}' not in class_names\")\n",
    "            self.remap[old_idx] = self.class_to_idx[new_name]\n",
    "\n",
    "        # targets in merged label space (needed for weighting/splitting)\n",
    "        self.targets = [self.remap[y] for y in self.base.targets]\n",
    "\n",
    "        # optional but useful: ImageFolder-like samples in merged label space\n",
    "        self.samples = [(path, self.remap[y]) for (path, y) in self.base.samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img, old_y = self.base[i]     # img is PIL image (transform not yet applied)\n",
    "        y = self.remap[old_y]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de4ffd74-70cb-4b82-8356-c86460db1776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):  #currently not used\n",
    "    \"\"\"\n",
    "    Multi-class Focal Loss for logits.\n",
    "    - inputs: logits of shape (N, C)\n",
    "    - targets: int labels of shape (N,)\n",
    "    Supports optional class weights and label smoothing.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=1.5, weight=None, label_smoothing=0.0, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight  # tensor shape (C,) on same device as inputs\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # log_probs: (N, C)\n",
    "        log_probs = F.log_softmax(inputs, dim=1)\n",
    "        probs = log_probs.exp()\n",
    "\n",
    "        n_classes = inputs.size(1)\n",
    "\n",
    "        if self.label_smoothing > 0.0:\n",
    "            # Smoothed one-hot targets: (N, C)\n",
    "            with torch.no_grad():\n",
    "                true_dist = torch.zeros_like(inputs)\n",
    "                true_dist.fill_(self.label_smoothing / (n_classes - 1))\n",
    "                true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - self.label_smoothing)\n",
    "\n",
    "            # CE per sample: -sum(y * logp)\n",
    "            ce = -(true_dist * log_probs).sum(dim=1)\n",
    "\n",
    "            # p_t: prob assigned to the (smoothed) target distribution\n",
    "            # Use expected probability under true_dist\n",
    "            pt = (true_dist * probs).sum(dim=1).clamp(min=1e-8, max=1.0)\n",
    "\n",
    "            if self.weight is not None:\n",
    "                # Expected class weight under smoothed target distribution\n",
    "                w = (true_dist * self.weight.unsqueeze(0)).sum(dim=1)\n",
    "                ce = ce * w\n",
    "\n",
    "        else:\n",
    "            # Standard CE per sample using the true class index\n",
    "            ce = F.nll_loss(log_probs, targets, weight=self.weight, reduction=\"none\")\n",
    "            pt = probs.gather(1, targets.unsqueeze(1)).squeeze(1).clamp(min=1e-8, max=1.0)\n",
    "\n",
    "        focal_factor = (1.0 - pt) ** self.gamma\n",
    "        loss = focal_factor * ce\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b69c130c-4a9c-4b8a-8610-339278ebb31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedSubset(Dataset):   #obsolete, used for 7 emotion calsses version\n",
    "    \"\"\"\n",
    "    A wrapper for a Dataset subset that applies a transform \n",
    "    to the samples without modifying the underlying dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y                     #x is the feature, y is the label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "def get_data_loaders(data_dir, batch_size=32, train_split=TRAIN_SPLIT, val_split=VAL_SPLIT, num_workers=0, seed=SEED):  #used for 7 emotion classes version\n",
    "    \"\"\"\n",
    "    Loads data from a single directory organized by class folders and \n",
    "    splits it into Train, Val, and Test sets.\n",
    "    \"\"\"\n",
    "    #Ensure split makes sense\n",
    "    assert 0 < train_split < 1\n",
    "    assert 0 <= val_split < 1\n",
    "    assert train_split + val_split < 1, \"train_split + val_split must be < 1\"\n",
    "    \n",
    "    IMG_SIZE = 224\n",
    "\n",
    "    #Standard Resnet Normalization\n",
    "    normalization = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # 1. Define separate transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10), \n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),   #consider changing this for warmup stage\n",
    "        transforms.ToTensor(),\n",
    "        normalization\n",
    "    ])\n",
    "\n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        normalization\n",
    "    ])\n",
    "\n",
    "    # 2. Load the entire dataset using ImageFolder\n",
    "    # This automatically uses folder names as labels\n",
    "    full_dataset = datasets.ImageFolder(root=data_dir)\n",
    "\n",
    "    #check step 2 executed correctly\n",
    "    print(\"Classes:\", full_dataset.classes)\n",
    "    print(\"Counts:\", np.bincount(full_dataset.targets))\n",
    "\n",
    "    # 3. Calculate the lengths for the split\n",
    "    total_size = len(full_dataset)\n",
    "    train_size = int(total_size*train_split)\n",
    "    val_size = int(total_size*val_split)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    # 4. Perform the random split\n",
    "    train_raw, val_raw, test_raw = random_split(\n",
    "        full_dataset, [train_size, val_size, test_size], \n",
    "        generator = torch.Generator().manual_seed(seed)\n",
    "    )\n",
    "    \n",
    "   # 5. Wrap subsets with their respective transforms\n",
    "    # This prevents the \"leakage\" where val/test get training augmentations\n",
    "    train_subset = TransformedSubset(train_raw, transform=train_transform)\n",
    "    val_subset = TransformedSubset(val_raw, transform=val_test_transform)\n",
    "    test_subset = TransformedSubset(test_raw, transform=val_test_transform)\n",
    "    \n",
    "    # 6. Create Dataloaders\n",
    "    # Note: pin_memory=True is helpful if you are using your CUDA GPU\n",
    "    train_loader = DataLoader(\n",
    "        train_subset, batch_size = batch_size, shuffle=True, \n",
    "        num_workers=num_workers, pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_subset, batch_size = batch_size, shuffle = False, \n",
    "        num_workers=num_workers, pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_subset, batch_size = batch_size, shuffle = False, \n",
    "        num_workers=num_workers, pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader, train_raw, val_raw, test_raw, full_dataset\n",
    "\n",
    "def get_data_loaders_merged(data_dir, batch_size=32, train_split=0.7, val_split=0.15, num_workers=0, seed=SEED):\n",
    "    IMG_SIZE = 224\n",
    "    normalization = transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        normalization\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalization\n",
    "    ])\n",
    "\n",
    "    full_train_ds = MergedImageFolder(root=data_dir, merge_map=MERGE_MAP, class_names=NEW_CLASSES, transform=train_transform)\n",
    "    full_val_ds = MergedImageFolder(root=data_dir, merge_map=MERGE_MAP, class_names=NEW_CLASSES, transform=val_transform)\n",
    "\n",
    "    N = len(full_train_ds)\n",
    "    train_size = int(N*train_split)\n",
    "    val_size = int(N*val_split)\n",
    "    test_size = N - train_size - val_size\n",
    "\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    perm = torch.randperm(N, generator=g).tolist()\n",
    "\n",
    "    train_idx = perm[:train_size]\n",
    "    val_idx = perm[train_size:train_size + val_size]\n",
    "    test_idx = perm[train_size + val_size:]\n",
    "    \n",
    "    train_ds = Subset(full_train_ds, train_idx)\n",
    "    val_ds   = Subset(full_val_ds, val_idx)\n",
    "    test_ds  = Subset(full_val_ds, test_idx)\n",
    "\n",
    "    # split size sanity check\n",
    "    assert len(train_idx) + len(val_idx) + len(test_idx) == N\n",
    "    print(\"Split sizes:\", len(train_idx), len(val_idx), len(test_idx))\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, \n",
    "                              pin_memory=torch.cuda.is_available())\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
    "                              pin_memory=torch.cuda.is_available())\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
    "                              pin_memory=torch.cuda.is_available())\n",
    "\n",
    "    return train_loader, val_loader, test_loader, train_ds, val_ds, test_ds, full_train_ds, full_val_ds\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b0d3385-8d5b-42c8-ab0a-1510db1560bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def make_class_weighted_criterion(train_subset, full_dataset, device, use_focal=True,\n",
    "                                  gamma=1.5, label_smoothing=0.0):\n",
    "    \"\"\"\n",
    "    train_subset: the Subset you are training on (train_ds)\n",
    "    full_dataset: the dataset that owns the labels (full_train_ds or full_val_ds)\n",
    "    \"\"\"\n",
    "\n",
    "    #gaurd\n",
    "    if not hasattr(train_subset, \"indices\"):\n",
    "        raise TypeError(\"train_subset must be a torch.utils.data.Subset (needs .indices)\")\n",
    "    \n",
    "    # train_subset.indices are indices into full_dataset\n",
    "    train_labels = [full_dataset.targets[i] for i in train_subset.indices]\n",
    "    counts = Counter(train_labels)\n",
    "\n",
    "    num_classes = len(full_dataset.classes)\n",
    "    class_counts = torch.tensor([counts.get(i, 0) for i in range(num_classes)], dtype=torch.float)\n",
    "    class_counts = class_counts.clamp_min(1.0)           #prevent accidental division by 0\n",
    "\n",
    "    weights = 1.0 / class_counts\n",
    "    weights = weights / weights.mean() # normalize avg weight ~ 1\n",
    "    weights = weights.to(device)\n",
    "\n",
    "    #sanity check\n",
    "    print(\"Class counts (train only):\", class_counts.cpu().numpy())\n",
    "    print(\"Weights:\", weights.cpu().numpy())\n",
    "    print(\"Classes:\", full_dataset.classes)\n",
    "\n",
    "    if use_focal:\n",
    "        criterion = FocalLoss(gamma=gamma, weight=weights, label_smoothing=label_smoothing)   #use weight = None if using focal to prevent overcorrection\n",
    "    else:\n",
    "        # fallback: standard CE\n",
    "        try:\n",
    "            criterion = torch.nn.CrossEntropyLoss(weight=weights, label_smoothing=label_smoothing)\n",
    "        except TypeError:\n",
    "            criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "    return criterion, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "358e02ad-5be8-4b72-8982-8e64c8a8cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL CREATION\n",
    "# ============================================================================\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "\n",
    "# helper for manipulating dropout\n",
    "def set_head_dropout(model, p: float):\n",
    "     # Supports model.fc = nn.Sequential(Dropout, Linear, ...)\n",
    "    if hasattr(model, \"fc\") and isinstance(model.fc, nn.Sequential):\n",
    "        for m in model.fc:\n",
    "            if isinstance(m, nn.Dropout):\n",
    "                m.p = p\n",
    "\n",
    "def create_resnet34_model(num_classes=5, pretrained=True, dropout_p=0.3):    #change dropout to 0.5 in case of overfitting\n",
    "    \"\"\"Create ResNet-34 for emotion classification\"\"\"\n",
    "    \n",
    "    # Load pretrained weights or None\n",
    "    weights = ResNet34_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "    model = resnet34(weights=weights)\n",
    "    \n",
    "    # Replace the classifier head with dropout + linear layer\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(dropout_p),                                        #randomly deactivates a subset of neurons during training to reduce overfitting (increase to 0.5?)\n",
    "        nn.Linear(model.fc.in_features, num_classes)\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "    \n",
    "# =========================\n",
    "# Freeze/unfreeze utilities\n",
    "# =========================\n",
    "def set_backbone_trainable(model, trainable: bool):\n",
    "    # Everything except the final fc\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(\"fc.\"):\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = trainable\n",
    "            \n",
    "# =======================================\n",
    "# Build optimizer with discriminative LRs:\n",
    "# =======================================\n",
    "def build_optimizer(model, lr_backbone=1e-4, lr_head=5e-4, weight_decay=1e-4):\n",
    "    backbone_params = []\n",
    "    head_params = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if name.startswith(\"fc.\"):\n",
    "            head_params.append(p)\n",
    "        else:\n",
    "            backbone_params.append(p)\n",
    "    \n",
    "    return optim.AdamW(\n",
    "        [\n",
    "            {\"params\": backbone_params, \"lr\": lr_backbone},\n",
    "            {\"params\": head_params, \"lr\": lr_head},\n",
    "        ],\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "# =======================================\n",
    "# Helper functions\n",
    "# =======================================\n",
    "def count_trainable_params(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "785ca5c6-c17b-4384-8ed2-fe861fa1d9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "#flip_TTA helper function\n",
    "\n",
    "@torch.no_grad()\n",
    "def forward_tta(model, images, use_flip_tta: bool):\n",
    "    if not use_flip_tta:\n",
    "        return model(images)\n",
    "    \n",
    "    logits = model(images)\n",
    "    logits_flip = model(torch.flip(images, dims=[3])) # dim=3 is width (horizontal flip)\n",
    "    return 0.5 * (logits + logits_flip)\n",
    "\n",
    "# Helper function \n",
    "def set_backbone_bn_eval(model):\n",
    "    # put only backbone BN (BatchNorm) layers into eval mode\n",
    "    for name, m in model.named_modules():\n",
    "        if not name.startswith(\"fc.\") and isinstance(m, nn.BatchNorm2d):\n",
    "            m.eval()\n",
    "\n",
    "def _mixup_data(x, y, alpha=0.2):                #Not currently used\n",
    "    \"\"\"\n",
    "    Returns mixed inputs, paired targets, and lambda.\n",
    "    If alpha <= 0, returns original inputs/targets.\n",
    "    \"\"\"\n",
    "    if alpha <= 0:\n",
    "        return x, y, None, 1.0\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size, device=x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1.0 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "    \n",
    "def train_one_epoch(model, train_loader, criterion, optimizer,              #set mixup_alpha to 0.0 to disable\n",
    "                    device, freeze_backbone_bn=False, mixup_alpha=0.0):     #criterion = the loss function (e.g. nn.CrossEntropyLoss)\n",
    "    \n",
    "    \"\"\"Train for one epoch\"\"\"                                               #optimizer updates the model's parameters (torch.optim.Adam or SGD)\n",
    "    model.train()\n",
    "\n",
    "     # Freeze backbone BN only if requested (usually during head-only warmup)\n",
    "    if freeze_backbone_bn:\n",
    "        set_backbone_bn_eval(model)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')         #pbar = progress bar -- for visualization\n",
    "    \n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device, non_blocking = True) \n",
    "        labels = labels.to(device, non_blocking = True)    #Moves tensors to GPU or CPU\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)           #Before computing gradients for this batch, we reset previous gradients to zero.\n",
    "\n",
    "        # Mixup\n",
    "        images, y_a, y_b, lam = _mixup_data(images, labels, alpha=mixup_alpha)\n",
    "        \n",
    "        outputs = model(images)                         #feeds batch of images throught the network. Outputs tensor of shape (batch_size, num_classes),\n",
    "                                                        #   each row containing raw logits (unnormalized scores) for each class\n",
    "        # Loss\n",
    "        if y_b is None:\n",
    "            loss = criterion(outputs, y_a)              #y_a is original labels\n",
    "        else:\n",
    "            loss = lam * criterion(outputs, y_a) + (1.0 - lam) * criterion(outputs, y_b)\n",
    "            \n",
    "        #loss = criterion(outputs, labels)               #Computes loss, returns scalar loss (average over the batch)\n",
    "        loss.backward()                                 #Back propagation\n",
    "        optimizer.step()                                #Uses the gradients to update the model's parameters (adjusts weights to reduce loss)\n",
    "        \n",
    "        # Update loss\n",
    "        running_loss += loss.item()                    #loss.item() converts the PyTorch scalar tensor to a Python float.\n",
    "        num_batches += 1\n",
    "        \n",
    "        # \"Approx\" accuracy for progress display\n",
    "        # Use y_a (orignial labels) even if mixed. This is only a monitor metric\n",
    "        _, predicted = outputs.max(1)                   #predicted gets the index of the maximum (argmax), i.e., the predicted class ID.\n",
    "        total += y_a.size(0)                         #We increase total to track how many samples we've seen so far in the epoch.\n",
    "        correct += predicted.eq(y_a).sum().item()    #correct = total number of correctly classified samples\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': running_loss / num_batches,\n",
    "            'acc': 100. * correct / total\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / num_batches            #computes mean loss per batch\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc                       #Scalars: epoch_loss: average training loss for this epoch.\n",
    "                                                       #         epoch_acc: average training accuracy (percent).\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device, use_flip_tta=False):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()                                       #Puts model in evaluation mode (turns off Dropout and uses running means instaed of batch stats)\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():                                           #Pytorch does not compute gradients (optimizes performance & memory), avoids backdrop storage overhead\n",
    "        for images, labels in tqdm(val_loader, desc='Validation'):\n",
    "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            \n",
    "            outputs = forward_tta(model, images, use_flip_tta)  #modified\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                scheduler, device, num_epochs=50, patience=7, freeze_epochs=5):\n",
    "    \"\"\"Main training loop with early stopping\"\"\"\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Phase A: freeze backbone\n",
    "    set_backbone_trainable(model, trainable=False)\n",
    "    set_backbone_bn_eval(model)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 60)\n",
    "        \n",
    "        # Unfreeze after warmup and rebuild optimizer/scheduler once\n",
    "        if epoch == freeze_epochs:\n",
    "            print(\"→ Unfreezing backbone and switching to discriminative learning rates\")\n",
    "\n",
    "            set_backbone_trainable(model, trainable=True)\n",
    "\n",
    "            # Optional: let BN adapt once backbone is trainable\n",
    "            # set_backbone_bn_train(model)\n",
    "\n",
    "            optimizer = build_optimizer(model, lr_backbone=1e-4, lr_head=5e-4, weight_decay=1e-4)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode='min', factor=0.5, patience=2\n",
    "            )\n",
    "            \n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device,\n",
    "            freeze_backbone_bn=(epoch < freeze_epochs),\n",
    "            mixup_alpha=(0.0 if epoch >= freeze_epochs else 0.0)\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device, use_flip_tta=False) #set to false to disable flip_tta\n",
    "        \n",
    "        # Update learning rate\n",
    "        if epoch >= freeze_epochs:\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "        \n",
    "        if len(optimizer.param_groups) == 1:\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'Learning Rate (head-only): {lr:.6f}')\n",
    "        else:\n",
    "            lr_backbone = optimizer.param_groups[0]['lr']\n",
    "            lr_head = optimizer.param_groups[1]['lr']\n",
    "            print(f'Learning Rate (backbone): {lr_backbone:.6f} | (head): {lr_head:.6f}')\n",
    "        \n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f'✓ New best model saved! (Val Acc: {val_acc:.2f}%)')\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'No improvement. Patience: {patience_counter}/{patience}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f'\\n⚠ Early stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, val_losses, train_accs, val_accs)      \n",
    "\n",
    "    \n",
    "    return model, best_val_acc                                                   #returns model:FIXME current model(wights from last epoch trained, \n",
    "                                                                                #   not necessarily the best. (OK?--YES) and returns best validation acc encountered \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "805c9761-01ed-40b6-81a3-00742256c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(train_losses, label='Train Loss', marker='o')\n",
    "    ax1.plot(val_losses, label='Val Loss', marker='s')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(train_accs, label='Train Acc', marker='o')\n",
    "    ax2.plot(val_accs, label='Val Acc', marker='s')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\n✓ Training history plot saved as 'training_history.png'\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.title('Confusion Matrix', fontsize=16, pad=20)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Confusion matrix saved as 'confusion_matrix.png'\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b56d0780-266a-4170-9bb1-0812a8463145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Model Evaluation\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_model(model, test_loader, device, class_names, use_flip_tta=False):\n",
    "    \"\"\"Evaluate model on test set and plot confusion matrix.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = forward_tta(model, images, use_flip_tta)\n",
    "            predicted = outputs.argmax(dim=1)\n",
    "            \n",
    "            # Accumulate predictions and labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Accuracy update\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc = 100.0 * correct / total\n",
    "    print(f\"\\n✓ Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "    # Sanity checks\n",
    "    num_classes = len(class_names)\n",
    "    assert min(all_labels) >= 0 and max(all_labels) < num_classes\n",
    "    assert min(all_preds)  >= 0 and max(all_preds)  < num_classes\n",
    "\n",
    "    # Per-class metrics (high value)\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(all_labels, all_preds, class_names)\n",
    "    \n",
    "    return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff14e246-5c7b-4473-820a-f4710f65dc3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to define main()\n",
      "About to call main() explicitly\n",
      "\n",
      "============================================================\n",
      "FACIAL EMOTION RECOGNITION - ResNet34\n",
      "============================================================\n",
      "Device: cuda\n",
      "Batch Size: 32\n",
      "Initial Head Learning Rate: 0.001\n",
      "Number of Epochs: 50\n",
      "Validation Split: 0.15\n",
      "============================================================\n",
      "Merged classes: ['Angry', 'Disgust', 'Happy', 'LowAffect', 'Arousal']\n",
      "MERGE_MAP keys example: ['Angry', 'Disgust', 'Happy', 'Neutral', 'Sad']\n",
      "Split sizes: 36149 7746 7747\n",
      "Merged class_to_idx: {'Angry': 0, 'Disgust': 1, 'Happy': 2, 'LowAffect': 3, 'Arousal': 4}\n",
      "Merged classes: ['Angry', 'Disgust', 'Happy', 'LowAffect', 'Arousal']\n",
      "\n",
      "Data loaders ready:\n",
      "  Train batches: 1130\n",
      "  Val batches: 243\n",
      "  Test batches: 243\n",
      "One batch: torch.Size([32, 3, 224, 224]) torch.Size([32])\n",
      "\n",
      "============================================================\n",
      "INITIALIZING MODEL\n",
      "============================================================\n",
      "✓ ResNet-34 model created (pretrained on ImageNet)\n",
      "✓ Final layer modified for 5 classes\n",
      "Initial head dropout: 0.3\n",
      "Class counts (train only): [ 4808.  3008.  6472. 13318.  8543.]\n",
      "Weights: [1.1723021  1.8738128  0.87089443 0.42321885 0.6597716 ]\n",
      "Classes: ['Angry', 'Disgust', 'Happy', 'LowAffect', 'Arousal']\n",
      "Class weights: [1.1723021  1.8738128  0.87089443 0.42321885 0.6597716 ]\n",
      "Criterion: CrossEntropyLoss()\n",
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "\n",
      "Epoch 1/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████| 1130/1130 [06:27<00:00,  2.92it/s, loss=1.58, acc=32.1]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:02<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (head-only): 0.001000\n",
      "Train Loss: 1.5791, Train Acc: 32.06%\n",
      "Val Loss: 1.5215, Val Acc: 39.54%\n",
      "✓ New best model saved! (Val Acc: 39.54%)\n",
      "\n",
      "Epoch 2/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████| 1130/1130 [05:42<00:00,  3.30it/s, loss=1.54, acc=34.8]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:02<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (head-only): 0.001000\n",
      "Train Loss: 1.5421, Train Acc: 34.83%\n",
      "Val Loss: 1.4802, Val Acc: 37.96%\n",
      "No improvement. Patience: 1/7\n",
      "\n",
      "Epoch 3/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████| 1130/1130 [05:46<00:00,  3.26it/s, loss=1.54, acc=34.9]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:04<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (head-only): 0.001000\n",
      "Train Loss: 1.5434, Train Acc: 34.86%\n",
      "Val Loss: 1.4929, Val Acc: 38.81%\n",
      "No improvement. Patience: 2/7\n",
      "\n",
      "Epoch 4/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████| 1130/1130 [05:55<00:00,  3.18it/s, loss=1.54, acc=34.6]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:05<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (head-only): 0.001000\n",
      "Train Loss: 1.5424, Train Acc: 34.59%\n",
      "Val Loss: 1.4943, Val Acc: 36.79%\n",
      "No improvement. Patience: 3/7\n",
      "\n",
      "Epoch 5/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████| 1130/1130 [05:54<00:00,  3.19it/s, loss=1.53, acc=35.6]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:04<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (head-only): 0.001000\n",
      "Train Loss: 1.5324, Train Acc: 35.62%\n",
      "Val Loss: 1.4768, Val Acc: 37.19%\n",
      "No improvement. Patience: 4/7\n",
      "\n",
      "Epoch 6/50\n",
      "------------------------------------------------------------\n",
      "→ Unfreezing backbone and switching to discriminative learning rates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████| 1130/1130 [06:35<00:00,  2.86it/s, loss=1.18, acc=59.2]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:05<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000100 | (head): 0.000500\n",
      "Train Loss: 1.1827, Train Acc: 59.15%\n",
      "Val Loss: 1.0104, Val Acc: 67.45%\n",
      "✓ New best model saved! (Val Acc: 67.45%)\n",
      "\n",
      "Epoch 7/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████| 1130/1130 [06:27<00:00,  2.91it/s, loss=1.01, acc=67.9]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:05<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000100 | (head): 0.000500\n",
      "Train Loss: 1.0121, Train Acc: 67.94%\n",
      "Val Loss: 0.9914, Val Acc: 69.38%\n",
      "✓ New best model saved! (Val Acc: 69.38%)\n",
      "\n",
      "Epoch 8/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [06:14<00:00,  3.02it/s, loss=0.966, acc=70.4]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:06<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000100 | (head): 0.000500\n",
      "Train Loss: 0.9658, Train Acc: 70.44%\n",
      "Val Loss: 0.9538, Val Acc: 70.17%\n",
      "✓ New best model saved! (Val Acc: 70.17%)\n",
      "\n",
      "Epoch 9/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████| 1130/1130 [06:15<00:00,  3.01it/s, loss=0.93, acc=72.1]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:05<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000100 | (head): 0.000500\n",
      "Train Loss: 0.9298, Train Acc: 72.09%\n",
      "Val Loss: 0.9892, Val Acc: 69.91%\n",
      "No improvement. Patience: 1/7\n",
      "\n",
      "Epoch 10/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [06:16<00:00,  3.00it/s, loss=0.901, acc=73.7]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:06<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000100 | (head): 0.000500\n",
      "Train Loss: 0.9006, Train Acc: 73.66%\n",
      "Val Loss: 0.9520, Val Acc: 69.82%\n",
      "No improvement. Patience: 2/7\n",
      "\n",
      "Epoch 11/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [06:14<00:00,  3.02it/s, loss=0.878, acc=74.6]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:06<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000100 | (head): 0.000500\n",
      "Train Loss: 0.8776, Train Acc: 74.62%\n",
      "Val Loss: 0.9596, Val Acc: 69.44%\n",
      "No improvement. Patience: 3/7\n",
      "\n",
      "Epoch 12/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [06:13<00:00,  3.03it/s, loss=0.853, acc=75.8]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:05<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000100 | (head): 0.000500\n",
      "Train Loss: 0.8527, Train Acc: 75.79%\n",
      "Val Loss: 0.9655, Val Acc: 73.41%\n",
      "✓ New best model saved! (Val Acc: 73.41%)\n",
      "\n",
      "Epoch 13/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████| 1130/1130 [06:13<00:00,  3.03it/s, loss=0.83, acc=77]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:06<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000050 | (head): 0.000250\n",
      "Train Loss: 0.8301, Train Acc: 76.96%\n",
      "Val Loss: 0.9580, Val Acc: 72.99%\n",
      "No improvement. Patience: 1/7\n",
      "\n",
      "Epoch 14/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [06:14<00:00,  3.01it/s, loss=0.747, acc=80.8]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:05<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000050 | (head): 0.000250\n",
      "Train Loss: 0.7467, Train Acc: 80.83%\n",
      "Val Loss: 0.9547, Val Acc: 72.22%\n",
      "No improvement. Patience: 2/7\n",
      "\n",
      "Epoch 15/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [06:14<00:00,  3.02it/s, loss=0.714, acc=82.5]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:06<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000050 | (head): 0.000250\n",
      "Train Loss: 0.7138, Train Acc: 82.50%\n",
      "Val Loss: 0.9832, Val Acc: 71.69%\n",
      "No improvement. Patience: 3/7\n",
      "\n",
      "Epoch 16/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [06:14<00:00,  3.02it/s, loss=0.686, acc=83.9]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:06<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000025 | (head): 0.000125\n",
      "Train Loss: 0.6860, Train Acc: 83.86%\n",
      "Val Loss: 0.9918, Val Acc: 70.17%\n",
      "No improvement. Patience: 4/7\n",
      "\n",
      "Epoch 17/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [06:13<00:00,  3.03it/s, loss=0.628, acc=86.5]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:06<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000025 | (head): 0.000125\n",
      "Train Loss: 0.6281, Train Acc: 86.47%\n",
      "Val Loss: 1.0323, Val Acc: 73.44%\n",
      "✓ New best model saved! (Val Acc: 73.44%)\n",
      "\n",
      "Epoch 18/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [06:14<00:00,  3.02it/s, loss=0.604, acc=87.5]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:05<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000025 | (head): 0.000125\n",
      "Train Loss: 0.6038, Train Acc: 87.54%\n",
      "Val Loss: 1.0545, Val Acc: 72.26%\n",
      "No improvement. Patience: 1/7\n",
      "\n",
      "Epoch 19/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [09:27<00:00,  1.99it/s, loss=0.584, acc=88.6]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [02:00<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000013 | (head): 0.000063\n",
      "Train Loss: 0.5844, Train Acc: 88.59%\n",
      "Val Loss: 1.0675, Val Acc: 72.82%\n",
      "No improvement. Patience: 2/7\n",
      "\n",
      "Epoch 20/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [07:42<00:00,  2.44it/s, loss=0.558, acc=89.8]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:05<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000013 | (head): 0.000063\n",
      "Train Loss: 0.5582, Train Acc: 89.81%\n",
      "Val Loss: 1.0792, Val Acc: 72.88%\n",
      "No improvement. Patience: 3/7\n",
      "\n",
      "Epoch 21/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [06:14<00:00,  3.02it/s, loss=0.542, acc=90.8]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:05<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000013 | (head): 0.000063\n",
      "Train Loss: 0.5416, Train Acc: 90.83%\n",
      "Val Loss: 1.1012, Val Acc: 73.22%\n",
      "No improvement. Patience: 4/7\n",
      "\n",
      "Epoch 22/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [06:12<00:00,  3.03it/s, loss=0.532, acc=91.3]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:05<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000006 | (head): 0.000031\n",
      "Train Loss: 0.5322, Train Acc: 91.26%\n",
      "Val Loss: 1.1108, Val Acc: 72.55%\n",
      "No improvement. Patience: 5/7\n",
      "\n",
      "Epoch 23/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [06:14<00:00,  3.02it/s, loss=0.515, acc=92.1]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:05<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000006 | (head): 0.000031\n",
      "Train Loss: 0.5145, Train Acc: 92.06%\n",
      "Val Loss: 1.1203, Val Acc: 73.20%\n",
      "No improvement. Patience: 6/7\n",
      "\n",
      "Epoch 24/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████| 1130/1130 [06:13<00:00,  3.02it/s, loss=0.51, acc=92.4]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [01:05<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000006 | (head): 0.000031\n",
      "Train Loss: 0.5098, Train Acc: 92.39%\n",
      "Val Loss: 1.1149, Val Acc: 73.24%\n",
      "No improvement. Patience: 7/7\n",
      "\n",
      "⚠ Early stopping triggered at epoch 24\n",
      "\n",
      "✓ Training history plot saved as 'training_history.png'\n",
      "\n",
      "============================================================\n",
      "LOADING BEST MODEL FOR TESTING\n",
      "============================================================\n",
      "✓ Loaded best model (Val Acc: 73.44%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|███████████████████████████████████████████████████████████████████████| 243/243 [01:58<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Test Accuracy: 74.88%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry     0.5689    0.6701    0.6154       961\n",
      "     Disgust     0.5798    0.5753    0.5775       657\n",
      "       Happy     0.8623    0.8751    0.8687      1417\n",
      "   LowAffect     0.7821    0.7607    0.7713      2808\n",
      "     Arousal     0.7821    0.7369    0.7588      1904\n",
      "\n",
      "    accuracy                         0.7488      7747\n",
      "   macro avg     0.7150    0.7236    0.7183      7747\n",
      "weighted avg     0.7532    0.7488    0.7502      7747\n",
      "\n",
      "✓ Confusion matrix saved as 'confusion_matrix.png'\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE - SUMMARY\n",
      "============================================================\n",
      "Best Validation Accuracy: 73.44%\n",
      "Test Accuracy: 74.88%\n",
      "Model saved as: best_model.pth\n",
      "============================================================\n",
      "returned from main\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"About to define main()\")\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FACIAL EMOTION RECOGNITION - ResNet34\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "    print(f\"Initial Head Learning Rate: {LEARNING_RATE}\")\n",
    "    print(f\"Number of Epochs: {NUM_EPOCHS}\")\n",
    "    print(f\"Validation Split: {VAL_SPLIT}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    #debug\n",
    "    print(\"Merged classes:\", NEW_CLASSES)\n",
    "    print(\"MERGE_MAP keys example:\", list(MERGE_MAP.keys())[:5] if isinstance(MERGE_MAP, dict) else MERGE_MAP)\n",
    "\n",
    "\n",
    "    # Load data\n",
    "    train_loader, val_loader, test_loader, train_ds, val_ds, test_ds, full_train_ds, full_val_ds = get_data_loaders_merged(\n",
    "        data_dir = DATA_DIR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        train_split = TRAIN_SPLIT,\n",
    "        val_split = VAL_SPLIT, \n",
    "        num_workers = NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    class_names = full_train_ds.classes\n",
    "\n",
    "     #for debugging\n",
    "    print(\"Merged class_to_idx:\", full_train_ds.class_to_idx)\n",
    "    print(\"Merged classes:\", full_train_ds.classes)\n",
    "\n",
    "    \n",
    "    print(f\"\\nData loaders ready:\")\n",
    "    print(f\"  Train batches: {len(train_loader)}\")\n",
    "    print(f\"  Val batches: {len(val_loader)}\")\n",
    "    print(f\"  Test batches: {len(test_loader)}\")\n",
    "\n",
    "    #for debugging\n",
    "    images, labels = next(iter(train_loader))\n",
    "    print(\"One batch:\", images.shape, labels.shape)\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"INITIALIZING MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    model = create_resnet34_model(num_classes=len(class_names), pretrained=True)\n",
    "    model = model.to(DEVICE)\n",
    "    print(\"✓ ResNet-34 model created (pretrained on ImageNet)\")\n",
    "    print(f\"✓ Final layer modified for {len(class_names)} classes\")\n",
    "\n",
    "    #Dropout test\n",
    "    print(\"Initial head dropout:\", model.fc[0].p)\n",
    "\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion, class_weights = make_class_weighted_criterion(\n",
    "        train_ds, full_train_ds, DEVICE, \n",
    "        use_focal=False,                           \n",
    "        gamma = 1.5,\n",
    "        label_smoothing=0.07         #0.05 if not using Focal\n",
    "    )\n",
    "    print(\"Class weights:\", class_weights.detach().cpu().numpy())\n",
    "    print(\"Criterion:\", criterion)\n",
    "   # print(\"Loss: FocalLoss(gamma=1.5, label_smoothing=0.0)\")\n",
    "    \n",
    "    optimizer = optim.AdamW(model.fc.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)  #added weight decay\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=2\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model, best_val_acc = train_model(\n",
    "        model, train_loader, val_loader, criterion, \n",
    "        optimizer, scheduler, DEVICE, \n",
    "        num_epochs=NUM_EPOCHS, patience=PATIENCE,\n",
    "        freeze_epochs=5\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    # Load best model for testing\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LOADING BEST MODEL FOR TESTING\")\n",
    "    print(\"=\" * 60)\n",
    "    state_dict = torch.load('best_model.pth', map_location=DEVICE)        #ensures always load to correct device\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(DEVICE)                                                      #not necessary, ensures model is loaded to proper device\n",
    "    print(f\"✓ Loaded best model (Val Acc: {best_val_acc:.2f}%)\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_acc = evaluate_model(model, test_loader, DEVICE, class_names)\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING COMPLETE - SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"Model saved as: best_model.pth\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "print(\"About to call main() explicitly\")\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "main()\n",
    "print(\"returned from main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427c85f-12a2-42f6-a143-a89bea904243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python torch-gpu",
   "language": "python",
   "name": "venv_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
