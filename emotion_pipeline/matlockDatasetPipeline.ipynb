{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a7c372a-2c47-45ef-841d-961ae372c75a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cu126\n",
      "12.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68877072-44f0-48c3-a0bf-2deb31756705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete Training Script for Facial Emotion Recognition using ResNet-34\n",
    "5 emotions: angry, disgust, happy, low affect (neutral & sad), arousal (fear & suprise)\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "from torchvision import transforms, datasets\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score  #accuracy score used?\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Paths - MODIFY THESE\n",
    "DATA_DIR = Path(r\"C:\\Users\\avyes\\DrAIv2\\emotion_pipeline\\master_dataset\") # Directory with 7 emotion folders\n",
    "\n",
    "\n",
    "#Ensure path exists\n",
    "assert DATA_DIR.exists(), f\"DATA_DIR not found: {DATA_DIR}\"\n",
    "\n",
    "#Merge neutral/sad, fear/surprise\n",
    "NEW_CLASSES = [\"Angry\", \"Disgust\", \"Happy\", \"LowAffect\", \"Arousal\"]\n",
    "\n",
    "MERGE_MAP = {\n",
    "    \"Angry\": \"Angry\",\n",
    "    \"Disgust\": \"Disgust\",\n",
    "    \"Happy\": \"Happy\",\n",
    "    \"Neutral\": \"LowAffect\",\n",
    "    \"Sad\": \"LowAffect\",\n",
    "    \"Fear\": \"Arousal\",\n",
    "    \"Surprise\": \"Arousal\",\n",
    "}\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3       # This name might be misleading bc of variable learning rate (check later)\n",
    "NUM_WORKERS = 0            #set to 0 if you get issues on Windows\n",
    "VAL_SPLIT = 0.15\n",
    "TRAIN_SPLIT = 0.70\n",
    "PATIENCE = 7  # For early stopping\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 42\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819bd11d-cfc7-4a6a-a9b7-bee19bc03ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergedImageFolder(Dataset):\n",
    "    def __init__(self, root, merge_map, class_names, transform=None):\n",
    "        self.base = datasets.ImageFolder(root=root)  # original folder labels\n",
    "        self.merge_map = merge_map\n",
    "        self.transform = transform\n",
    "\n",
    "        # merged class interface (ImageFolder-like)\n",
    "        self.classes = list(class_names)\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
    "\n",
    "        # original class index -> name\n",
    "        idx_to_class = {v: k for k, v in self.base.class_to_idx.items()}\n",
    "\n",
    "        # remap: old_idx -> new_idx\n",
    "        self.remap = {}\n",
    "        for old_idx, old_name in idx_to_class.items():\n",
    "            if old_name not in self.merge_map:\n",
    "                raise KeyError(f\"Missing '{old_name}' in merge_map keys: {list(self.merge_map.keys())}\")\n",
    "            new_name = self.merge_map[old_name]\n",
    "            if new_name not in self.class_to_idx:\n",
    "                raise KeyError(f\"merge_map maps '{old_name}' -> '{new_name}', but '{new_name}' not in class_names\")\n",
    "            self.remap[old_idx] = self.class_to_idx[new_name]\n",
    "\n",
    "        # targets in merged label space (needed for weighting/splitting)\n",
    "        self.targets = [self.remap[y] for y in self.base.targets]\n",
    "\n",
    "        # optional but useful: ImageFolder-like samples in merged label space\n",
    "        self.samples = [(path, self.remap[y]) for (path, y) in self.base.samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img, old_y = self.base[i]     # img is PIL image (transform not yet applied)\n",
    "        y = self.remap[old_y]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de4ffd74-70cb-4b82-8356-c86460db1776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):  #currently not used\n",
    "    \"\"\"\n",
    "    Multi-class Focal Loss for logits.\n",
    "    - inputs: logits of shape (N, C)\n",
    "    - targets: int labels of shape (N,)\n",
    "    Supports optional class weights and label smoothing.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=1.5, weight=None, label_smoothing=0.0, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight  # tensor shape (C,) on same device as inputs\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # log_probs: (N, C)\n",
    "        log_probs = F.log_softmax(inputs, dim=1)\n",
    "        probs = log_probs.exp()\n",
    "\n",
    "        n_classes = inputs.size(1)\n",
    "\n",
    "        if self.label_smoothing > 0.0:\n",
    "            # Smoothed one-hot targets: (N, C)\n",
    "            with torch.no_grad():\n",
    "                true_dist = torch.zeros_like(inputs)\n",
    "                true_dist.fill_(self.label_smoothing / (n_classes - 1))\n",
    "                true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - self.label_smoothing)\n",
    "\n",
    "            # CE per sample: -sum(y * logp)\n",
    "            ce = -(true_dist * log_probs).sum(dim=1)\n",
    "\n",
    "            # p_t: prob assigned to the (smoothed) target distribution\n",
    "            # Use expected probability under true_dist\n",
    "            pt = (true_dist * probs).sum(dim=1).clamp(min=1e-8, max=1.0)\n",
    "\n",
    "            if self.weight is not None:\n",
    "                # Expected class weight under smoothed target distribution\n",
    "                w = (true_dist * self.weight.unsqueeze(0)).sum(dim=1)\n",
    "                ce = ce * w\n",
    "\n",
    "        else:\n",
    "            # Standard CE per sample using the true class index\n",
    "            ce = F.nll_loss(log_probs, targets, weight=self.weight, reduction=\"none\")\n",
    "            pt = probs.gather(1, targets.unsqueeze(1)).squeeze(1).clamp(min=1e-8, max=1.0)\n",
    "\n",
    "        focal_factor = (1.0 - pt) ** self.gamma\n",
    "        loss = focal_factor * ce\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b69c130c-4a9c-4b8a-8610-339278ebb31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedSubset(Dataset):   #obsolete, used for 7 emotion calsses version\n",
    "    \"\"\"\n",
    "    A wrapper for a Dataset subset that applies a transform \n",
    "    to the samples without modifying the underlying dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y                     #x is the feature, y is the label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "def get_data_loaders(data_dir, batch_size=32, train_split=TRAIN_SPLIT, val_split=VAL_SPLIT, num_workers=0, seed=SEED):  #used for 7 emotion classes version\n",
    "    \"\"\"\n",
    "    Loads data from a single directory organized by class folders and \n",
    "    splits it into Train, Val, and Test sets.\n",
    "    \"\"\"\n",
    "    #Ensure split makes sense\n",
    "    assert 0 < train_split < 1\n",
    "    assert 0 <= val_split < 1\n",
    "    assert train_split + val_split < 1, \"train_split + val_split must be < 1\"\n",
    "    \n",
    "    IMG_SIZE = 224\n",
    "\n",
    "    #Standard Resnet Normalization\n",
    "    normalization = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # 1. Define separate transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10), \n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),   #consider changing this for warmup stage\n",
    "        transforms.ToTensor(),\n",
    "        normalization\n",
    "    ])\n",
    "\n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        normalization\n",
    "    ])\n",
    "\n",
    "    # 2. Load the entire dataset using ImageFolder\n",
    "    # This automatically uses folder names as labels\n",
    "    full_dataset = datasets.ImageFolder(root=data_dir)\n",
    "\n",
    "    #check step 2 executed correctly\n",
    "    print(\"Classes:\", full_dataset.classes)\n",
    "    print(\"Counts:\", np.bincount(full_dataset.targets))\n",
    "\n",
    "    # 3. Calculate the lengths for the split\n",
    "    total_size = len(full_dataset)\n",
    "    train_size = int(total_size*train_split)\n",
    "    val_size = int(total_size*val_split)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    # 4. Perform the random split\n",
    "    train_raw, val_raw, test_raw = random_split(\n",
    "        full_dataset, [train_size, val_size, test_size], \n",
    "        generator = torch.Generator().manual_seed(seed)\n",
    "    )\n",
    "    \n",
    "   # 5. Wrap subsets with their respective transforms\n",
    "    # This prevents the \"leakage\" where val/test get training augmentations\n",
    "    train_subset = TransformedSubset(train_raw, transform=train_transform)\n",
    "    val_subset = TransformedSubset(val_raw, transform=val_test_transform)\n",
    "    test_subset = TransformedSubset(test_raw, transform=val_test_transform)\n",
    "    \n",
    "    # 6. Create Dataloaders\n",
    "    # Note: pin_memory=True is helpful if you are using your CUDA GPU\n",
    "    train_loader = DataLoader(\n",
    "        train_subset, batch_size = batch_size, shuffle=True, \n",
    "        num_workers=num_workers, pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_subset, batch_size = batch_size, shuffle = False, \n",
    "        num_workers=num_workers, pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_subset, batch_size = batch_size, shuffle = False, \n",
    "        num_workers=num_workers, pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader, train_raw, val_raw, test_raw, full_dataset\n",
    "\n",
    "def get_data_loaders_merged(data_dir, batch_size=32, train_split=0.7, val_split=0.15, num_workers=0, seed=SEED):\n",
    "    IMG_SIZE = 224\n",
    "    normalization = transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        normalization\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalization\n",
    "    ])\n",
    "\n",
    "    full_train_ds = MergedImageFolder(root=data_dir, merge_map=MERGE_MAP, class_names=NEW_CLASSES, transform=train_transform)\n",
    "    full_val_ds = MergedImageFolder(root=data_dir, merge_map=MERGE_MAP, class_names=NEW_CLASSES, transform=val_transform)\n",
    "\n",
    "    N = len(full_train_ds)\n",
    "    train_size = int(N*train_split)\n",
    "    val_size = int(N*val_split)\n",
    "    test_size = N - train_size - val_size\n",
    "\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    perm = torch.randperm(N, generator=g).tolist()\n",
    "\n",
    "    train_idx = perm[:train_size]\n",
    "    val_idx = perm[train_size:train_size + val_size]\n",
    "    test_idx = perm[train_size + val_size:]\n",
    "    \n",
    "    train_ds = Subset(full_train_ds, train_idx)\n",
    "    val_ds   = Subset(full_val_ds, val_idx)\n",
    "    test_ds  = Subset(full_val_ds, test_idx)\n",
    "\n",
    "    # split size sanity check\n",
    "    assert len(train_idx) + len(val_idx) + len(test_idx) == N\n",
    "    print(\"Split sizes:\", len(train_idx), len(val_idx), len(test_idx))\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, \n",
    "                              pin_memory=torch.cuda.is_available())\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
    "                              pin_memory=torch.cuda.is_available())\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
    "                              pin_memory=torch.cuda.is_available())\n",
    "\n",
    "    return train_loader, val_loader, test_loader, train_ds, val_ds, test_ds, full_train_ds, full_val_ds\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b0d3385-8d5b-42c8-ab0a-1510db1560bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def make_class_weighted_criterion(train_subset, full_dataset, device, use_focal=True,\n",
    "                                  gamma=1.5, label_smoothing=0.0):\n",
    "    \"\"\"\n",
    "    train_subset: the Subset you are training on (train_ds)\n",
    "    full_dataset: the dataset that owns the labels (full_train_ds or full_val_ds)\n",
    "    \"\"\"\n",
    "\n",
    "    #gaurd\n",
    "    if not hasattr(train_subset, \"indices\"):\n",
    "        raise TypeError(\"train_subset must be a torch.utils.data.Subset (needs .indices)\")\n",
    "    \n",
    "    # train_subset.indices are indices into full_dataset\n",
    "    train_labels = [full_dataset.targets[i] for i in train_subset.indices]\n",
    "    counts = Counter(train_labels)\n",
    "\n",
    "    num_classes = len(full_dataset.classes)\n",
    "    class_counts = torch.tensor([counts.get(i, 0) for i in range(num_classes)], dtype=torch.float)\n",
    "    class_counts = class_counts.clamp_min(1.0)           #prevent accidental division by 0\n",
    "\n",
    "    weights = 1.0 / class_counts\n",
    "    weights = weights / weights.mean() # normalize avg weight ~ 1\n",
    "    weights = weights.to(device)\n",
    "\n",
    "    #sanity check\n",
    "    print(\"Class counts (train only):\", class_counts.cpu().numpy())\n",
    "    print(\"Weights:\", weights.cpu().numpy())\n",
    "    print(\"Classes:\", full_dataset.classes)\n",
    "\n",
    "    if use_focal:\n",
    "        criterion = FocalLoss(gamma=gamma, weight=weights, label_smoothing=label_smoothing)   #use weight = None if using focal to prevent overcorrection\n",
    "    else:\n",
    "        # fallback: standard CE\n",
    "        try:\n",
    "            criterion = torch.nn.CrossEntropyLoss(weight=weights, label_smoothing=label_smoothing)\n",
    "        except TypeError:\n",
    "            criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "    return criterion, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "358e02ad-5be8-4b72-8982-8e64c8a8cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL CREATION\n",
    "# ============================================================================\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "\n",
    "# helper for manipulating dropout\n",
    "def set_head_dropout(model, p: float):\n",
    "     # Supports model.fc = nn.Sequential(Dropout, Linear, ...)\n",
    "    if hasattr(model, \"fc\") and isinstance(model.fc, nn.Sequential):\n",
    "        for m in model.fc:\n",
    "            if isinstance(m, nn.Dropout):\n",
    "                m.p = p\n",
    "\n",
    "def create_resnet34_model(num_classes=5, pretrained=True, dropout_p=0.4):    #change dropout to 0.5 in case of overfitting\n",
    "    \"\"\"Create ResNet-34 for emotion classification\"\"\"\n",
    "    \n",
    "    # Load pretrained weights or None\n",
    "    weights = ResNet34_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "    model = resnet34(weights=weights)\n",
    "    \n",
    "    # Replace the classifier head with dropout + linear layer\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(dropout_p),                                        #randomly deactivates a subset of neurons during training to reduce overfitting (increase to 0.5?)\n",
    "        nn.Linear(model.fc.in_features, num_classes)\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "    \n",
    "# =========================\n",
    "# Freeze/unfreeze utilities\n",
    "# =========================\n",
    "def set_backbone_trainable(model, trainable: bool):\n",
    "    # Everything except the final fc\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(\"fc.\"):\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = trainable\n",
    "            \n",
    "# =======================================\n",
    "# Build optimizer with discriminative LRs:\n",
    "# =======================================\n",
    "def build_optimizer(model, lr_backbone=1e-4, lr_head=5e-4, weight_decay=1e-4):\n",
    "    backbone_params = []\n",
    "    head_params = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if name.startswith(\"fc.\"):\n",
    "            head_params.append(p)\n",
    "        else:\n",
    "            backbone_params.append(p)\n",
    "    \n",
    "    return optim.AdamW(\n",
    "        [\n",
    "            {\"params\": backbone_params, \"lr\": lr_backbone},\n",
    "            {\"params\": head_params, \"lr\": lr_head},\n",
    "        ],\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "# =======================================\n",
    "# Helper functions\n",
    "# =======================================\n",
    "def count_trainable_params(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "785ca5c6-c17b-4384-8ed2-fe861fa1d9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "#flip_TTA helper function\n",
    "\n",
    "#@torch.no_grad()\n",
    "#def forward_tta(model, images, use_flip_tta: bool):\n",
    "#    if not use_flip_tta:\n",
    "#        return model(images)\n",
    "#    \n",
    "#    logits = model(images)\n",
    "#    logits_flip = model(torch.flip(images, dims=[3])) # dim=3 is width (horizontal flip)\n",
    "#    return 0.5 * (logits + logits_flip)\n",
    "\n",
    "@torch.no_grad()\n",
    "def forward_tta(model, images, use_flip_tta: bool):\n",
    "    \"\"\"\n",
    "    Enhanced Test-Time Augmentation with multiple crops\n",
    "    \"\"\"\n",
    "    if not use_flip_tta:\n",
    "        return model(images)\n",
    "    \n",
    "    batch_size = images.size(0)\n",
    "    all_logits = []\n",
    "    \n",
    "    # 1. Original image\n",
    "    all_logits.append(model(images))\n",
    "    \n",
    "    # 2. Horizontal flip\n",
    "    all_logits.append(model(torch.flip(images, dims=[3])))\n",
    "    \n",
    "    # 3. Five crops: top-left, top-right, bottom-left, bottom-right, center\n",
    "    # Each crop is 200x200 from the 224x224 image, then resized back\n",
    "    h, w = images.size(2), images.size(3)\n",
    "    crop_size = int(0.9 * min(h, w))  # 90% of image size\n",
    "    \n",
    "    # Top-left\n",
    "    crop_tl = images[:, :, :crop_size, :crop_size]\n",
    "    crop_tl_resized = F.interpolate(crop_tl, size=(h, w), mode='bilinear', align_corners=False)\n",
    "    all_logits.append(model(crop_tl_resized))\n",
    "    \n",
    "    # Top-right\n",
    "    crop_tr = images[:, :, :crop_size, -crop_size:]\n",
    "    crop_tr_resized = F.interpolate(crop_tr, size=(h, w), mode='bilinear', align_corners=False)\n",
    "    all_logits.append(model(crop_tr_resized))\n",
    "    \n",
    "    # Bottom-left\n",
    "    crop_bl = images[:, :, -crop_size:, :crop_size]\n",
    "    crop_bl_resized = F.interpolate(crop_bl, size=(h, w), mode='bilinear', align_corners=False)\n",
    "    all_logits.append(model(crop_bl_resized))\n",
    "    \n",
    "    # Bottom-right\n",
    "    crop_br = images[:, :, -crop_size:, -crop_size:]\n",
    "    crop_br_resized = F.interpolate(crop_br, size=(h, w), mode='bilinear', align_corners=False)\n",
    "    all_logits.append(model(crop_br_resized))\n",
    "    \n",
    "    # Center crop\n",
    "    margin = (h - crop_size) // 2\n",
    "    crop_center = images[:, :, margin:margin+crop_size, margin:margin+crop_size]\n",
    "    crop_center_resized = F.interpolate(crop_center, size=(h, w), mode='bilinear', align_corners=False)\n",
    "    all_logits.append(model(crop_center_resized))\n",
    "    \n",
    "    # Average all predictions (7 total)\n",
    "    return torch.stack(all_logits).mean(dim=0)\n",
    "\n",
    "# Helper function \n",
    "def set_backbone_bn_eval(model):\n",
    "    # put only backbone BN (BatchNorm) layers into eval mode\n",
    "    for name, m in model.named_modules():\n",
    "        if not name.startswith(\"fc.\") and isinstance(m, nn.BatchNorm2d):\n",
    "            m.eval()\n",
    "\n",
    "def _mixup_data(x, y, alpha=0.2):                #Not currently used\n",
    "    \"\"\"\n",
    "    Returns mixed inputs, paired targets, and lambda.\n",
    "    If alpha <= 0, returns original inputs/targets.\n",
    "    \"\"\"\n",
    "    if alpha <= 0:\n",
    "        return x, y, None, 1.0\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size, device=x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1.0 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "    \n",
    "def train_one_epoch(model, train_loader, criterion, optimizer,              #set mixup_alpha to 0.0 to disable\n",
    "                    device, freeze_backbone_bn=False, mixup_alpha=0.0):     #criterion = the loss function (e.g. nn.CrossEntropyLoss)\n",
    "    \n",
    "    \"\"\"Train for one epoch\"\"\"                                               #optimizer updates the model's parameters (torch.optim.Adam or SGD)\n",
    "    model.train()\n",
    "\n",
    "     # Freeze backbone BN only if requested (usually during head-only warmup)\n",
    "    if freeze_backbone_bn:\n",
    "        set_backbone_bn_eval(model)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')         #pbar = progress bar -- for visualization\n",
    "    \n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device, non_blocking = True) \n",
    "        labels = labels.to(device, non_blocking = True)    #Moves tensors to GPU or CPU\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)           #Before computing gradients for this batch, we reset previous gradients to zero.\n",
    "\n",
    "        # Mixup\n",
    "        images, y_a, y_b, lam = _mixup_data(images, labels, alpha=mixup_alpha)\n",
    "        \n",
    "        outputs = model(images)                         #feeds batch of images throught the network. Outputs tensor of shape (batch_size, num_classes),\n",
    "                                                        #   each row containing raw logits (unnormalized scores) for each class\n",
    "        # Loss\n",
    "        if y_b is None:\n",
    "            loss = criterion(outputs, y_a)              #y_a is original labels\n",
    "        else:\n",
    "            loss = lam * criterion(outputs, y_a) + (1.0 - lam) * criterion(outputs, y_b)\n",
    "            \n",
    "        #loss = criterion(outputs, labels)               #Computes loss, returns scalar loss (average over the batch)\n",
    "        loss.backward()                                 #Back propagation\n",
    "        optimizer.step()                                #Uses the gradients to update the model's parameters (adjusts weights to reduce loss)\n",
    "        \n",
    "        # Update loss\n",
    "        running_loss += loss.item()                    #loss.item() converts the PyTorch scalar tensor to a Python float.\n",
    "        num_batches += 1\n",
    "        \n",
    "        # \"Approx\" accuracy for progress display\n",
    "        # Use y_a (orignial labels) even if mixed. This is only a monitor metric\n",
    "        _, predicted = outputs.max(1)                   #predicted gets the index of the maximum (argmax), i.e., the predicted class ID.\n",
    "        total += y_a.size(0)                         #We increase total to track how many samples we've seen so far in the epoch.\n",
    "        correct += predicted.eq(y_a).sum().item()    #correct = total number of correctly classified samples\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': running_loss / num_batches,\n",
    "            'acc': 100. * correct / total\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / num_batches            #computes mean loss per batch\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc                       #Scalars: epoch_loss: average training loss for this epoch.\n",
    "                                                       #         epoch_acc: average training accuracy (percent).\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device, use_flip_tta=False):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()                                       #Puts model in evaluation mode (turns off Dropout and uses running means instaed of batch stats)\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():                                           #Pytorch does not compute gradients (optimizes performance & memory), avoids backdrop storage overhead\n",
    "        for images, labels in tqdm(val_loader, desc='Validation'):\n",
    "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            \n",
    "            outputs = forward_tta(model, images, use_flip_tta)  #modified\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                scheduler, device, num_epochs=50, patience=7, freeze_epochs=5):\n",
    "    \"\"\"Main training loop with early stopping\"\"\"\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Phase A: freeze backbone\n",
    "    set_backbone_trainable(model, trainable=False)\n",
    "    set_backbone_bn_eval(model)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 60)\n",
    "        \n",
    "        # Unfreeze after warmup and rebuild optimizer/scheduler once\n",
    "        if epoch == freeze_epochs:\n",
    "            print(\"→ Unfreezing backbone and switching to discriminative learning rates\")\n",
    "\n",
    "            set_backbone_trainable(model, trainable=True)\n",
    "\n",
    "            # Optional: let BN adapt once backbone is trainable\n",
    "            # set_backbone_bn_train(model)\n",
    "\n",
    "            optimizer = build_optimizer(model, lr_backbone=1e-4, lr_head=5e-4, weight_decay=1e-4)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode='min', factor=0.5, patience=2\n",
    "            )\n",
    "            \n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device,\n",
    "            freeze_backbone_bn=(epoch < freeze_epochs),\n",
    "            mixup_alpha=(0.0 if epoch >= freeze_epochs else 0.0)\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device, use_flip_tta=False) #set to false to disable flip_tta\n",
    "        \n",
    "        # Update learning rate\n",
    "        if epoch >= freeze_epochs:\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "        \n",
    "        if len(optimizer.param_groups) == 1:\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'Learning Rate (head-only): {lr:.6f}')\n",
    "        else:\n",
    "            lr_backbone = optimizer.param_groups[0]['lr']\n",
    "            lr_head = optimizer.param_groups[1]['lr']\n",
    "            print(f'Learning Rate (backbone): {lr_backbone:.6f} | (head): {lr_head:.6f}')\n",
    "        \n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f'✓ New best model saved! (Val Acc: {val_acc:.2f}%)')\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'No improvement. Patience: {patience_counter}/{patience}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f'\\n⚠ Early stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, val_losses, train_accs, val_accs)      \n",
    "\n",
    "    \n",
    "    return model, best_val_acc                                                   #returns model:FIXME current model(wights from last epoch trained, \n",
    "                                                                                #   not necessarily the best. (OK?--YES) and returns best validation acc encountered \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "805c9761-01ed-40b6-81a3-00742256c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(train_losses, label='Train Loss', marker='o')\n",
    "    ax1.plot(val_losses, label='Val Loss', marker='s')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(train_accs, label='Train Acc', marker='o')\n",
    "    ax2.plot(val_accs, label='Val Acc', marker='s')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\n✓ Training history plot saved as 'training_history.png'\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.title('Confusion Matrix', fontsize=16, pad=20)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Confusion matrix saved as 'confusion_matrix.png'\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b56d0780-266a-4170-9bb1-0812a8463145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Model Evaluation\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_model(model, test_loader, device, class_names, use_flip_tta=False):\n",
    "    \"\"\"Evaluate model on test set and plot confusion matrix.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = forward_tta(model, images, use_flip_tta)\n",
    "            predicted = outputs.argmax(dim=1)\n",
    "            \n",
    "            # Accumulate predictions and labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Accuracy update\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc = 100.0 * correct / total\n",
    "    print(f\"\\n✓ Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "    # Sanity checks\n",
    "    num_classes = len(class_names)\n",
    "    assert min(all_labels) >= 0 and max(all_labels) < num_classes\n",
    "    assert min(all_preds)  >= 0 and max(all_preds)  < num_classes\n",
    "\n",
    "    # Per-class metrics (high value)\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(all_labels, all_preds, class_names)\n",
    "    \n",
    "    return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff14e246-5c7b-4473-820a-f4710f65dc3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to define main()\n",
      "About to call main() explicitly\n",
      "\n",
      "============================================================\n",
      "FACIAL EMOTION RECOGNITION - ResNet34\n",
      "============================================================\n",
      "Device: cuda\n",
      "Batch Size: 32\n",
      "Initial Head Learning Rate: 0.001\n",
      "Number of Epochs: 50\n",
      "Validation Split: 0.15\n",
      "============================================================\n",
      "Merged classes: ['Angry', 'Disgust', 'Happy', 'LowAffect', 'Arousal']\n",
      "MERGE_MAP keys example: ['Angry', 'Disgust', 'Happy', 'Neutral', 'Sad']\n",
      "Split sizes: 36149 7746 7747\n",
      "Merged class_to_idx: {'Angry': 0, 'Disgust': 1, 'Happy': 2, 'LowAffect': 3, 'Arousal': 4}\n",
      "Merged classes: ['Angry', 'Disgust', 'Happy', 'LowAffect', 'Arousal']\n",
      "\n",
      "Data loaders ready:\n",
      "  Train batches: 1130\n",
      "  Val batches: 243\n",
      "  Test batches: 243\n",
      "One batch: torch.Size([32, 3, 224, 224]) torch.Size([32])\n",
      "\n",
      "============================================================\n",
      "INITIALIZING MODEL\n",
      "============================================================\n",
      "✓ ResNet-34 model created (pretrained on ImageNet)\n",
      "✓ Final layer modified for 5 classes\n",
      "Initial head dropout: 0.4\n",
      "Class counts (train only): [ 4808.  3008.  6472. 13318.  8543.]\n",
      "Weights: [1.1723021  1.8738128  0.87089443 0.42321885 0.6597716 ]\n",
      "Classes: ['Angry', 'Disgust', 'Happy', 'LowAffect', 'Arousal']\n",
      "Class weights: [1.1723021  1.8738128  0.87089443 0.42321885 0.6597716 ]\n",
      "Criterion: CrossEntropyLoss()\n",
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "\n",
      "Epoch 1/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████| 1130/1130 [26:36<00:00,  1.41s/it, loss=1.6, acc=31.1]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [05:20<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (head-only): 0.001000\n",
      "Train Loss: 1.6026, Train Acc: 31.07%\n",
      "Val Loss: 1.5358, Val Acc: 38.26%\n",
      "✓ New best model saved! (Val Acc: 38.26%)\n",
      "\n",
      "Epoch 2/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████| 1130/1130 [17:45<00:00,  1.06it/s, loss=1.57, acc=33.1]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [02:59<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (head-only): 0.001000\n",
      "Train Loss: 1.5687, Train Acc: 33.12%\n",
      "Val Loss: 1.4982, Val Acc: 37.27%\n",
      "No improvement. Patience: 1/7\n",
      "\n",
      "Epoch 3/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████| 1130/1130 [16:25<00:00,  1.15it/s, loss=1.57, acc=33.2]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [02:50<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (head-only): 0.001000\n",
      "Train Loss: 1.5718, Train Acc: 33.19%\n",
      "Val Loss: 1.5110, Val Acc: 38.05%\n",
      "No improvement. Patience: 2/7\n",
      "\n",
      "Epoch 4/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████| 1130/1130 [15:11<00:00,  1.24it/s, loss=1.57, acc=33.2]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [02:46<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (head-only): 0.001000\n",
      "Train Loss: 1.5707, Train Acc: 33.16%\n",
      "Val Loss: 1.5112, Val Acc: 33.02%\n",
      "No improvement. Patience: 3/7\n",
      "\n",
      "Epoch 5/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████| 1130/1130 [14:56<00:00,  1.26it/s, loss=1.56, acc=33.9]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [02:46<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (head-only): 0.001000\n",
      "Train Loss: 1.5587, Train Acc: 33.88%\n",
      "Val Loss: 1.4955, Val Acc: 36.16%\n",
      "No improvement. Patience: 4/7\n",
      "\n",
      "Epoch 6/50\n",
      "------------------------------------------------------------\n",
      "→ Unfreezing backbone and switching to discriminative learning rates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████| 1130/1130 [21:08<00:00,  1.12s/it, loss=1.21, acc=59]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [02:48<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000100 | (head): 0.000500\n",
      "Train Loss: 1.2060, Train Acc: 59.03%\n",
      "Val Loss: 1.0421, Val Acc: 67.47%\n",
      "✓ New best model saved! (Val Acc: 67.47%)\n",
      "\n",
      "Epoch 7/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████| 1130/1130 [21:25<00:00,  1.14s/it, loss=1.04, acc=67.6]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [02:55<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000100 | (head): 0.000500\n",
      "Train Loss: 1.0377, Train Acc: 67.63%\n",
      "Val Loss: 1.0253, Val Acc: 69.49%\n",
      "✓ New best model saved! (Val Acc: 69.49%)\n",
      "\n",
      "Epoch 8/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████| 1130/1130 [21:18<00:00,  1.13s/it, loss=0.99, acc=70.4]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [02:55<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000100 | (head): 0.000500\n",
      "Train Loss: 0.9905, Train Acc: 70.38%\n",
      "Val Loss: 0.9861, Val Acc: 69.80%\n",
      "✓ New best model saved! (Val Acc: 69.80%)\n",
      "\n",
      "Epoch 9/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████| 1130/1130 [21:26<00:00,  1.14s/it, loss=0.96, acc=71.9]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [02:56<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000100 | (head): 0.000500\n",
      "Train Loss: 0.9599, Train Acc: 71.86%\n",
      "Val Loss: 1.0013, Val Acc: 69.98%\n",
      "✓ New best model saved! (Val Acc: 69.98%)\n",
      "\n",
      "Epoch 10/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████| 1130/1130 [21:15<00:00,  1.13s/it, loss=0.93, acc=73.5]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [02:57<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000100 | (head): 0.000500\n",
      "Train Loss: 0.9305, Train Acc: 73.49%\n",
      "Val Loss: 0.9798, Val Acc: 69.17%\n",
      "No improvement. Patience: 1/7\n",
      "\n",
      "Epoch 11/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [21:28<00:00,  1.14s/it, loss=0.907, acc=74.5]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [02:57<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000100 | (head): 0.000500\n",
      "Train Loss: 0.9070, Train Acc: 74.48%\n",
      "Val Loss: 0.9895, Val Acc: 70.68%\n",
      "✓ New best model saved! (Val Acc: 70.68%)\n",
      "\n",
      "Epoch 12/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [25:06<00:00,  1.33s/it, loss=0.884, acc=75.6]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [03:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000100 | (head): 0.000500\n",
      "Train Loss: 0.8841, Train Acc: 75.60%\n",
      "Val Loss: 0.9802, Val Acc: 71.48%\n",
      "✓ New best model saved! (Val Acc: 71.48%)\n",
      "\n",
      "Epoch 13/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [23:09<00:00,  1.23s/it, loss=0.859, acc=77.1]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [03:07<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000050 | (head): 0.000250\n",
      "Train Loss: 0.8587, Train Acc: 77.07%\n",
      "Val Loss: 0.9810, Val Acc: 72.46%\n",
      "✓ New best model saved! (Val Acc: 72.46%)\n",
      "\n",
      "Epoch 14/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [21:13<00:00,  1.13s/it, loss=0.778, acc=80.9]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [03:01<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000050 | (head): 0.000250\n",
      "Train Loss: 0.7777, Train Acc: 80.93%\n",
      "Val Loss: 0.9760, Val Acc: 71.43%\n",
      "No improvement. Patience: 1/7\n",
      "\n",
      "Epoch 15/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [21:22<00:00,  1.14s/it, loss=0.746, acc=82.4]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [03:05<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000050 | (head): 0.000250\n",
      "Train Loss: 0.7462, Train Acc: 82.39%\n",
      "Val Loss: 1.0125, Val Acc: 71.44%\n",
      "No improvement. Patience: 2/7\n",
      "\n",
      "Epoch 16/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████| 1130/1130 [19:13<00:00,  1.02s/it, loss=0.72, acc=83.7]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [02:18<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000050 | (head): 0.000250\n",
      "Train Loss: 0.7203, Train Acc: 83.71%\n",
      "Val Loss: 1.0156, Val Acc: 70.90%\n",
      "No improvement. Patience: 3/7\n",
      "\n",
      "Epoch 17/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [20:26<00:00,  1.09s/it, loss=0.696, acc=84.9]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [02:16<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000025 | (head): 0.000125\n",
      "Train Loss: 0.6963, Train Acc: 84.90%\n",
      "Val Loss: 1.0450, Val Acc: 72.41%\n",
      "No improvement. Patience: 4/7\n",
      "\n",
      "Epoch 18/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [20:20<00:00,  1.08s/it, loss=0.645, acc=87.4]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [02:25<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000025 | (head): 0.000125\n",
      "Train Loss: 0.6445, Train Acc: 87.40%\n",
      "Val Loss: 1.0620, Val Acc: 72.67%\n",
      "✓ New best model saved! (Val Acc: 72.67%)\n",
      "\n",
      "Epoch 19/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████| 1130/1130 [20:37<00:00,  1.10s/it, loss=0.62, acc=88.7]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [02:25<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000025 | (head): 0.000125\n",
      "Train Loss: 0.6202, Train Acc: 88.73%\n",
      "Val Loss: 1.0707, Val Acc: 72.75%\n",
      "✓ New best model saved! (Val Acc: 72.75%)\n",
      "\n",
      "Epoch 20/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [20:38<00:00,  1.10s/it, loss=0.607, acc=89.4]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [02:20<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000013 | (head): 0.000063\n",
      "Train Loss: 0.6072, Train Acc: 89.38%\n",
      "Val Loss: 1.0780, Val Acc: 72.95%\n",
      "✓ New best model saved! (Val Acc: 72.95%)\n",
      "\n",
      "Epoch 21/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████| 1130/1130 [21:18<00:00,  1.13s/it, loss=0.576, acc=91]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [02:36<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000013 | (head): 0.000063\n",
      "Train Loss: 0.5763, Train Acc: 91.01%\n",
      "Val Loss: 1.1008, Val Acc: 73.04%\n",
      "✓ New best model saved! (Val Acc: 73.04%)\n",
      "\n",
      "Epoch 22/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [22:10<00:00,  1.18s/it, loss=0.565, acc=91.7]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [03:13<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000013 | (head): 0.000063\n",
      "Train Loss: 0.5648, Train Acc: 91.65%\n",
      "Val Loss: 1.1271, Val Acc: 71.88%\n",
      "No improvement. Patience: 1/7\n",
      "\n",
      "Epoch 23/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [24:18<00:00,  1.29s/it, loss=0.553, acc=92.1]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [03:11<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000006 | (head): 0.000031\n",
      "Train Loss: 0.5527, Train Acc: 92.11%\n",
      "Val Loss: 1.1378, Val Acc: 72.27%\n",
      "No improvement. Patience: 2/7\n",
      "\n",
      "Epoch 24/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [28:55<00:00,  1.54s/it, loss=0.541, acc=92.8]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [03:34<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000006 | (head): 0.000031\n",
      "Train Loss: 0.5412, Train Acc: 92.77%\n",
      "Val Loss: 1.1364, Val Acc: 72.82%\n",
      "No improvement. Patience: 3/7\n",
      "\n",
      "Epoch 25/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████| 1130/1130 [22:22<00:00,  1.19s/it, loss=0.537, acc=93]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [02:24<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000006 | (head): 0.000031\n",
      "Train Loss: 0.5372, Train Acc: 93.05%\n",
      "Val Loss: 1.1399, Val Acc: 72.57%\n",
      "No improvement. Patience: 4/7\n",
      "\n",
      "Epoch 26/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [20:45<00:00,  1.10s/it, loss=0.531, acc=93.5]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [02:51<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000003 | (head): 0.000016\n",
      "Train Loss: 0.5315, Train Acc: 93.49%\n",
      "Val Loss: 1.1472, Val Acc: 72.17%\n",
      "No improvement. Patience: 5/7\n",
      "\n",
      "Epoch 27/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [22:13<00:00,  1.18s/it, loss=0.525, acc=93.6]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [02:29<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000003 | (head): 0.000016\n",
      "Train Loss: 0.5246, Train Acc: 93.57%\n",
      "Val Loss: 1.1469, Val Acc: 72.48%\n",
      "No improvement. Patience: 6/7\n",
      "\n",
      "Epoch 28/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████| 1130/1130 [23:34<00:00,  1.25s/it, loss=0.523, acc=93.7]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 243/243 [04:51<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (backbone): 0.000003 | (head): 0.000016\n",
      "Train Loss: 0.5229, Train Acc: 93.71%\n",
      "Val Loss: 1.1561, Val Acc: 72.82%\n",
      "No improvement. Patience: 7/7\n",
      "\n",
      "⚠ Early stopping triggered at epoch 28\n",
      "\n",
      "✓ Training history plot saved as 'training_history.png'\n",
      "\n",
      "============================================================\n",
      "LOADING BEST MODEL FOR TESTING\n",
      "============================================================\n",
      "✓ Loaded best model (Val Acc: 73.04%)\n",
      "\n",
      "============================================================\n",
      "STANDARD EVALUATION (No TTA)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|███████████████████████████████████████████████████████████████████████| 243/243 [05:07<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Test Accuracy: 74.65%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry     0.5941    0.6337    0.6133       961\n",
      "     Disgust     0.5260    0.6012    0.5611       657\n",
      "       Happy     0.8677    0.8751    0.8714      1417\n",
      "   LowAffect     0.7721    0.7650    0.7685      2808\n",
      "     Arousal     0.7903    0.7306    0.7593      1904\n",
      "\n",
      "    accuracy                         0.7465      7747\n",
      "   macro avg     0.7101    0.7211    0.7147      7747\n",
      "weighted avg     0.7511    0.7465    0.7482      7747\n",
      "\n",
      "✓ Confusion matrix saved as 'confusion_matrix.png'\n",
      "\n",
      "============================================================\n",
      "ENHANCED TTA EVALUATION (7 augmentations)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|███████████████████████████████████████████████████████████████████████| 243/243 [06:03<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Test Accuracy: 75.42%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry     0.5967    0.6389    0.6171       961\n",
      "     Disgust     0.5630    0.5921    0.5772       657\n",
      "       Happy     0.8696    0.8751    0.8723      1417\n",
      "   LowAffect     0.7735    0.7796    0.7765      2808\n",
      "     Arousal     0.7967    0.7411    0.7679      1904\n",
      "\n",
      "    accuracy                         0.7542      7747\n",
      "   macro avg     0.7199    0.7253    0.7222      7747\n",
      "weighted avg     0.7570    0.7542    0.7552      7747\n",
      "\n",
      "✓ Confusion matrix saved as 'confusion_matrix.png'\n",
      "\n",
      "============================================================\n",
      "FINAL COMPARISON\n",
      "============================================================\n",
      "Standard Accuracy: 74.65%\n",
      "TTA Accuracy:      75.42%\n",
      "Improvement:       +0.77%\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE - SUMMARY\n",
      "============================================================\n",
      "Best Validation Accuracy: 73.04%\n",
      "Test Accuracy: 75.42%\n",
      "Model saved as: best_model.pth\n",
      "============================================================\n",
      "returned from main\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"About to define main()\")\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FACIAL EMOTION RECOGNITION - ResNet34\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "    print(f\"Initial Head Learning Rate: {LEARNING_RATE}\")\n",
    "    print(f\"Number of Epochs: {NUM_EPOCHS}\")\n",
    "    print(f\"Validation Split: {VAL_SPLIT}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    #debug\n",
    "    print(\"Merged classes:\", NEW_CLASSES)\n",
    "    print(\"MERGE_MAP keys example:\", list(MERGE_MAP.keys())[:5] if isinstance(MERGE_MAP, dict) else MERGE_MAP)\n",
    "\n",
    "\n",
    "    # Load data\n",
    "    train_loader, val_loader, test_loader, train_ds, val_ds, test_ds, full_train_ds, full_val_ds = get_data_loaders_merged(\n",
    "        data_dir = DATA_DIR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        train_split = TRAIN_SPLIT,\n",
    "        val_split = VAL_SPLIT, \n",
    "        num_workers = NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    class_names = full_train_ds.classes\n",
    "\n",
    "     #for debugging\n",
    "    print(\"Merged class_to_idx:\", full_train_ds.class_to_idx)\n",
    "    print(\"Merged classes:\", full_train_ds.classes)\n",
    "\n",
    "    \n",
    "    print(f\"\\nData loaders ready:\")\n",
    "    print(f\"  Train batches: {len(train_loader)}\")\n",
    "    print(f\"  Val batches: {len(val_loader)}\")\n",
    "    print(f\"  Test batches: {len(test_loader)}\")\n",
    "\n",
    "    #for debugging\n",
    "    images, labels = next(iter(train_loader))\n",
    "    print(\"One batch:\", images.shape, labels.shape)\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"INITIALIZING MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    model = create_resnet34_model(num_classes=len(class_names), pretrained=True)\n",
    "    model = model.to(DEVICE)\n",
    "    print(\"✓ ResNet-34 model created (pretrained on ImageNet)\")\n",
    "    print(f\"✓ Final layer modified for {len(class_names)} classes\")\n",
    "\n",
    "    #Dropout test\n",
    "    print(\"Initial head dropout:\", model.fc[0].p)\n",
    "\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion, class_weights = make_class_weighted_criterion(\n",
    "        train_ds, full_train_ds, DEVICE, \n",
    "        use_focal=False,                           \n",
    "        gamma = 1.5,\n",
    "        label_smoothing=0.08         #0.05 if not using Focal\n",
    "    )\n",
    "    print(\"Class weights:\", class_weights.detach().cpu().numpy())\n",
    "    print(\"Criterion:\", criterion)\n",
    "   # print(\"Loss: FocalLoss(gamma=1.5, label_smoothing=0.0)\")\n",
    "    \n",
    "    optimizer = optim.AdamW(model.fc.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)  #added weight decay\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=2\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model, best_val_acc = train_model(\n",
    "        model, train_loader, val_loader, criterion, \n",
    "        optimizer, scheduler, DEVICE, \n",
    "        num_epochs=NUM_EPOCHS, patience=PATIENCE,\n",
    "        freeze_epochs=5\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    # Load best model for testing\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LOADING BEST MODEL FOR TESTING\")\n",
    "    print(\"=\" * 60)\n",
    "    state_dict = torch.load('best_model.pth', map_location=DEVICE)        #ensures always load to correct device\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(DEVICE)                                                      #not necessary, ensures model is loaded to proper device\n",
    "    print(f\"✓ Loaded best model (Val Acc: {best_val_acc:.2f}%)\")\n",
    "    \n",
    "    ## Evaluate on test set\n",
    "    #test_acc = evaluate_model(model, test_loader, DEVICE, class_names)\n",
    "\n",
    "    # Evaluate on test set WITHOUT TTA (baseline)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STANDARD EVALUATION (No TTA)\")\n",
    "    print(\"=\"*60)\n",
    "    test_acc_standard = evaluate_model(model, test_loader, DEVICE, class_names, use_flip_tta=False)\n",
    "\n",
    "    # Evaluate on test set WITH ENHANCED TTA\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ENHANCED TTA EVALUATION (7 augmentations)\")\n",
    "    print(\"=\"*60)\n",
    "    test_acc_tta = evaluate_model(model, test_loader, DEVICE, class_names, use_flip_tta=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Standard Accuracy: {test_acc_standard:.2f}%\")\n",
    "    print(f\"TTA Accuracy:      {test_acc_tta:.2f}%\")\n",
    "    print(f\"Improvement:       +{(test_acc_tta - test_acc_standard):.2f}%\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING COMPLETE - SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_acc_tta:.2f}%\")\n",
    "    print(f\"Model saved as: best_model.pth\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "print(\"About to call main() explicitly\")\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "main()\n",
    "print(\"returned from main\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (emotion_train)",
   "language": "python",
   "name": "emotion_train"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
